{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Y-TEzaA6ajL46LGMv45SdEkJVNB5Q4Fo","timestamp":1739792926231}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 웹크롤링(Web Crawling) 기초\n","* 웹 크롤링은 인터넷에서 특정 웹사이트나 웹페이지의 데이터를 자동으로 수집하는 기술\n","* 프로그램을 사용하여 웹페이지를 방문하고, HTML 문서, 텍스트, 이미지, 또는 기타 데이터를 추출하여 저장하는 방식으로 이루어짐\n","* 활용분야: 최신 뉴스 자동 수집, 전자상거래 웹사이트에서 상품 정보(가격, 리뷰, 상품 등) 수집, 데이터 분석을 포함\n"],"metadata":{"id":"KjMapVsZ_4Be"}},{"cell_type":"markdown","source":["### 웹 크롤링의 주요 절차\n","\n","1. 요청(Request): 데이터를 수집하고 싶은 웹사이트에 **접근 허락 요청**을하고, **가공되지 않은 HTML 코드(데이터)를** 가져옴\n","* requests 라이브러리가 사용\n","\n","2. 파싱(Parsing): 가공되지 않은 HTML 코드를 **원하는 조건으로 탐색하고 추출할 수 있는 HTML 코드로 변환**하고, 그 코드에서 원하는 태그와 관련 데이터를 추출\n","* BeautifulSoup 같은 라이브러리를 활용\n","* for문, findall() 함수,find()함수를 사용\n","\n","3. 데이터 저장: 추출한 데이터를 Pandas의 **데이터프레임(엑셀과 유사)으로 저장**\n","* Pandas 라이브러리를 이용하고, 그 데이터프레임을 **CSV 파일 형식으로 변환**해서 내드라이브의 폴더(예, mypython)으로 저장하는 방식이 일반적 (이번 워크숍에서는 다루지 않음)"],"metadata":{"id":"Q8U1gTJdIM5L"}},{"cell_type":"markdown","source":["### 간단한 Python 웹 크롤링 예시\n"],"metadata":{"id":"XTK0jI7VU1JS"}},{"cell_type":"markdown","source":["### 1. 드라이브 연동"],"metadata":{"id":"Xzu8OVIy5Lf6"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"cRgCSThzUfH4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739843526493,"user_tz":-540,"elapsed":19923,"user":{"displayName":"‍이두황(미디어학과)","userId":"16950066832571757101"}},"outputId":"22ee5daa-c58d-482f-accf-fd946899c018"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### 2. 필요한 라이브러리 활용"],"metadata":{"id":"hAWGU65l5U-o"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd"],"metadata":{"id":"dCdPFLbiKE0V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. 웹페이지에 접근요청하고 가공하지 않은 HTML 데이터를 가져오기\n","* requests 라이브러리를 이용해서 데이터를 수집하고자하는 웹 페이지에 HTTP 요청을 보내 응답 데이터를 확인\n","* 가공하지 않은 원본 HTML 코드를 받아오기\n","* 아래 예시 코드의 requests.get(URL) 이용하여 HTTP 요청 상태 확인(예: 200: 성공, 404: 실패)\n","* 아래 예시 코드의 html.text 이용해서 가공되지 않은 HTML 코드를 추출하고 문자열로 출력함\n","* 그러나 이 코드 출력값으로는 HTML 코드를 직접 탐색하거나 분석하지 않음\n"],"metadata":{"id":"w76nevIfEB-p"}},{"cell_type":"code","source":["# 크롤링할 Yes24 베스트셀러 페이지의 URL을 지정\n","URL ='https://www.yes24.com/Product/Category/BestSeller?categoryNumber=001&pageNumber=1&pageSize=24'\n","\n","# 지정된 URL에 HTTP GET 요청을 보내고, 응답 데이터(응답상태와 가공하지 않은 HTML 코드)를 html에 저장\n","html = requests.get(URL)\n","\n","# HTTP 응답 상태 코드 출력\n","print(html) # 출력값: <Response [200]>, 200는 HTTP 상태 코드: 200은 성공, 404: 요청한 페이지를 찾을 수 없음, 403: 접금금지, 500: 서버에러\n","\n","# 가져온 HTML 코드 출력(가공하지 않은 코드)\n","print(html.text)\n","# 예) <!DOCTYPE html><html><head><title>Yes24 베스트셀러</title></head><body><div class=\"book\">...</div></body></html>"],"metadata":{"id":"NgfZzkKdCzJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ### 5. 탐색하고 추출하기 쉬운 HTML 코드로 변환하고, 원하는 데이터 부분을 추출하기(파싱)\n","* BeautifulSoup 클래스를 이용해서 가져온 HTML 코드를 여러가지 조건에 맞게 원하는 부분만 추출하기(파싱)\n","* BeautifulSoup 클래스는 bs4(BeautifulSoup4) 라이브러리에 속해있음\n","* BeautifulSoup 클래스를 이용해서 가공하지 않은 HTML 코드를 탐색하고 추출하기 쉽게 변환\n","* 변환된 HTML 데이터를 조건에 맞게 여러 함수를 이용해 원하는 특정 태그와 데이터를 추출\n"],"metadata":{"id":"QG172cBLD2vr"}},{"cell_type":"code","source":["# BeautifulSoup 클래스 이용\n","from bs4 import BeautifulSoup\n","\n","#  가져온 HTML 코드(html.text)를 분석가능한 HTML 코드로 변환(= BeautifulSoup 객체라고도 함)\n","soup = BeautifulSoup(html.text,'html.parser') # 'html.parser': HTML 코드를 분석하기 위한 파서(분석도구)\n","print(soup)\n"],"metadata":{"id":"P3ouVQz4FohV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Yes24 베스트셀러 페이지의 서적명 추출\n","1. BeautifulSoup 객체에서 **<div> 태그 중 클래스가 'itemUnit'인 모든 요소** 를 찾고, 그 리스트를 **books 변수에 저장하기**\n","2. 문법형식: HTML코드객체.find_all('태그이름', class = '클래스이름')\n","3. for문, find_all(), find() 함수 사용\n","\n","* 예시코드: 베스트셀러 서적명 추출"],"metadata":{"id":"LhdXt2itSD2I"}},{"cell_type":"code","source":["# 위 변환한 HTML코드인 BeautifulSoup 객체(soup)에서\n","# find_all 이라는 함수를 이용해서 <div> 태그 중 클래스(class)가 'itemUnit'인 *모든 요소* 찾아, 그 리스트를 books 변수에 저장하기\n","books = soup.find_all('div', class_='itemUnit') #()안은 원하는 조건임\n","\n","# 각 책의 제목이 담긴 <a>태그 코드 반복해서 출력(for문 사용)\n","for book in books:\n","    a_tag_title = book.find('a', class_='gd_name')\n","    print(a_tag_title)\n","\n","# 클래스가 'itemUnit'인 모든 <div> 태그(books)의 HTML 블록를 하나씩 순회\n","# 그 <a> 태그 중 클래스(class)가 'gd_name'인 조건을 만족하는 태그를 하나씩 선택\n","# 책 제목이 담긴 <a> 태그 코드를 차례차례 a_tag_title에 저장\n","# a_tag_title(책 제목이 담긴 <a> 태그 코드)을 출력\n","\n","# 각 책의 제목 출력\n","for book in books:\n","    title = book.find('a', class_='gd_name').text\n","    print(title)\n","\n","# 클래스가 'itemUnit'인 모든 <div> 태그(books)의 HTML 블록를 하나씩 순회\n","# 그 <a> 태그 중 클래스(class)가 'gd_name'인 조건을 만족하는 태그를 하나씩 선택\n","# .text를 사용해 해당 <a> 태그의 텍스트(책 제목만)를 하나씩 추출\n","#  책제목을 하나씩 title 변수에 차례차례 저장\n","#  title(<a> 태그 코드 중 책 제목만)을 하나씩 출력\n","\n","\n","\n"],"metadata":{"id":"vq1UYojLJlID"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 참고: 코드 동작 예시\n","* books 리스트에 에 포함된 요소:"],"metadata":{"id":"z1TU4IipcWqL"}},{"cell_type":"code","source":["[<div class=\"itemUnit\">\n","    ...\n","    <a class=\"gd_name\" href=\"/Product/Goods/126590468\" ...>초역 부처의 말</a>\n","    ...\n","</div>,\n","<div class=\"itemUnit\">\n","    ...\n","    <a class='gd_name: href=\"/Product/Goods/142661358\" ...>사카모토 데이즈 19 트리플 특전판</a>\n","    ...\n","</div>,\n"," ...\n"," ...\n"," ...\n"," ...\n"," .\n"," .\n"," .\n"," </div>]\n"],"metadata":{"id":"L2UnprgDcTbH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Yes24 베스트셀러 페이지 크롤링해서 출력하는 파이썬 코드"],"metadata":{"id":"_tOhd_yUEMJ2"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# 크롤링할 Yes24 베스트셀러 페이지의 URL을 지정.\n","URL ='https://www.yes24.com/Product/Category/BestSeller?categoryNumber=001&pageNumber=1&pageSize=24'\n","\n","# 웹 페이지 요청: 지정된 URL에 HTTP GET 요청을 보내고, 응답 데이터를 response에 저장\n","html = requests.get(URL)\n","\n","# HTML 코드 파싱: 받은 HTML 코드(html.text)를 분석가능한 HTML 코드로 변환해서 원하는 데이터를 탐색하고 추출\n","soup = BeautifulSoup(html.text,'html.parser') # 'html.parser': HTML 코드를 분석하기 위한 파서(분석을 위한 도구)\n","\n","# 클래스가 'itemUnit'인 모든 <div> 태그 찾기: 그 리스트를 books 변수에 저장하기\n","books = soup.find_all('div', class_='itemUnit')\n","\n","# 각 책의 제목 출력\n","for book in books:\n","    title = book.find('a', class_='gd_name').text\n","    print(title)\n","\n","# 클래스가 'itemUnit'인 모든 <div> 태그(books)의 HTML 블록를 하나씩 순회\n","# 그 <a> 태그 중 클래스(class)가 'gd_name'인 조건을 만족하는 태그를 하나씩 선택\n","# .text를 사용해 해당 <a> 태그의 텍스트(책 제목)를 하나씩 추출\n","#  책제목 하나씩 title 변수에 차례차례 저장"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWnHC2g1EOhe","executionInfo":{"status":"ok","timestamp":1739843571709,"user_tz":-540,"elapsed":2883,"user":{"displayName":"‍이두황(미디어학과)","userId":"16950066832571757101"}},"outputId":"e071ac32-a36a-436a-c7b9-2a6379a8ff7a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["초역 부처의 말\n","사카모토 데이즈 19 트리플 특전판\n","소년이 온다\n","나의 히어로 아카데미아 42 아크릴 굿즈 한정판\n","어른의 행복은 조용하다\n","행동하지 않으면 인생은 바뀌지 않는다\n","하루 한 장 나의 어휘력을 위한 필사 노트\n","나의 히어로 아카데미아 42 일반판\n","황현필의 진보를 위한 역사\n","왜 그들만 부자가 되는가\n","급류\n","채식주의자\n","조국의 함성\n","모순\n","ETS 토익 정기시험 기출문제집 1000 Vol. 4 RC\n","작별하지 않는다\n","ETS 토익 정기시험 기출문제집 1000 Vol. 4 LC\n","내가 원하는 것을 나도 모를 때\n","STOP THE STEAL 대법원의 부정선거 은폐기록\n","내가 나라서 정말 좋아\n","마음의 기술\n","Go Go 카카오프렌즈 35\n","이처럼 사소한 것들\n","해커스 토익 기출 VOCA (보카)\n"]}]},{"cell_type":"markdown","source":["# 수집한 자료를 파일로 쓰고 불러오는 파이썬 코드\n","\n","*   파일 처리는 프로그래밍에서 중요한 작업\n","\n","*   텍스트 파일을 쓰고 저장하는 방법과, 저장된 파일을 불러와 읽는 방법"],"metadata":{"id":"dzx84PbrstBY"}},{"cell_type":"markdown","source":["# 1. 파일 쓰기와 저장하기\n","\n","*   파일 쓰기: 텍스트 데이터를 파일에 저장하기 위해서는 open() 함수를 사용하여 파일을 열고, write() 메서드를 사용해 내용을 원하는 폴더 씀\n","\n","*   파일을 다 쓴 후에는 자동으로 파일을 닫아주는 with 구문을 사용하는 것이 좋음"],"metadata":{"id":"f8XWU6cbs_Kl"}},{"cell_type":"code","source":["# 드라이브 연동한 상태로 파일에 저장 (책 제목을 하나씩 기록)\n","with open(\"/content/drive/MyDrive/mypython/bestseller_books.txt\", \"w\", encoding=\"utf-8\") as f:\n","    for book in books:\n","        title = book.find('a', class_='gd_name').text.strip()\n","        f.write(title + \"\\n\")  # 한 줄씩 저장 -  f는 파일을 가리키는 파일 객체(file object)\n","\n","print(\"파일이 저장되었습니다: /content/drive/MyDrive/mypython/bestseller_books.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RImDkItstK22","executionInfo":{"status":"ok","timestamp":1739843646438,"user_tz":-540,"elapsed":274,"user":{"displayName":"‍이두황(미디어학과)","userId":"16950066832571757101"}},"outputId":"e9e737e6-7f56-495d-e163-079dd757e1bf"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["파일이 저장되었습니다: /content/drive/MyDrive/mypython/bestseller_books.txt\n"]}]},{"cell_type":"markdown","source":["### 2. 파일 불러와서 읽기\n","\n","*   파일 읽기: 저장된 파일의 내용을 불러오려면, 파일을 읽기 모드(\"r\")로 열고, read() 메서드를 사용하여 내용을 읽어옴"],"metadata":{"id":"0xRNxm0bxJuR"}},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/mypython/bestseller_books.txt\", \"r\", encoding=\"utf-8\") as f:\n","    content = f.read() # f는 파일을 가리키는 파일 객체(file object)\n","\n","print(content)  # 전체 내용을 한 번에 출력"],"metadata":{"id":"7D9GdsnWxOi9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 숙제:. Yes24 베스트셀러 페이지 크롤링해서 **리스트로 저장**하고 출력하고 폴더에 저장하기"],"metadata":{"id":"XbS037R4oVKV"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# 크롤링할 Yes24 베스트셀러 페이지의 URL을 지정\n","URL = 'https://www.yes24.com/Product/Category/BestSeller?categoryNumber=001&pageNumber=1&pageSize=24'\n","\n","# 웹 페이지 요청: 지정된 URL에 HTTP GET 요청을 보내고, 응답 데이터(응답과 htmㅣ코드)를 html에 저장\n","html = requests.get(URL)\n","\n","# HTML 코드 파싱: 받은 HTML 코드(html.text)를 분석가능한 HTML 코드로 변환해서 원하는 데이터를 탐색하고 추출\n","soup = BeautifulSoup(html.text, 'html.parser')\n","\n","# 클래스가 'itemUnit'인 모든 <div> 태그 찾기: 그 리스트를 books 변수에 저장하기\n","books = soup.find_all('div', class_='itemUnit')\n","\n","# 빈 리스트 생성\n","book_titles = []\n","\n","# 각 책 제목을 리스트에 추가\n","for book in books: # 클래스가 'itemUnit'인 모든 <div> 태그(books)를 순회하면서 각각의 태그에 대해 ...\n","    title = book.find('a', class_='gd_name').text\n","    book_titles.append(title)\n","\n","# 클래스가 'itemUnit'인 모든 <div> 태그(books)의 HTML 블록를 하나씩 순회\n","# 그 <a> 태그 중 클래스(class)가 'gd_name'인 조건을 만족하는 태그를 하나씩 선택\n","# .text를 사용해 해당 <a> 태그의 텍스트(책 제목)를 하나씩 추출\n","#  책제목 하나씩 title 변수에 차례차례 저장\n","#  그책제목을 빈 리스트은 book_titles = [] 에 하나씩 추가\n","\n","# 리스트 출력\n","print(book_titles)\n"],"metadata":{"id":"aHYs9fofoVlR"},"execution_count":null,"outputs":[]}]}